ROOT_DIR: ./..
DATA_DIR: /mnt/nfs1/ryan/decouple/data
MODEL_DIR: ${ROOT_DIR}/models

num_proc: 16
seed: 0


train:
  do: True

  exp_name: "dynahate_lowdata_prompt_sft"

  model_path_or_name: "/home/ryan/decouple/models/unfiltered_exp9_3epoch_step3000_sft"
  out_directory: "/home/ryan/decouple/models/dataset_finetuned/dynahate_prompt_sft/unfiltered_exp9_3epoch_step3000_sft"

  # temp fix for tokenizer
  tokenizer_name: "/home/ryan/decouple/models/olmo_ckpt/contpretrain/exp_9_3epoch/unfiltered_exp9_3epoch/step3000-unsharded/hf"

  wandb:
    do: True
    project: "decouple"
    group: "OLMO-1B_dynahate_lowdata_sft"
    name: "unfiltered_exp9_3epoch_step3000_sft"

  max_seq_len: 256

  save_model: True

  apply_loss_mask: False # this is only used for tofu training

  training_args:
    per_device_train_batch_size: 4
    gradient_accumulation_steps: 32
    num_train_epochs: 2
    fp16: true
    learning_rate: 2.0e-06
    warmup_ratio: 0.03
    logging_steps: 5

  # note: this only works when exp_name uses eval
  eval:
    eval_steps: 40
    per_device_eval_batch_size: 16

  lora:
    do: False
    lora_modules: [ "q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj", "lm_head"] # for olmo and llama

