ROOT_DIR: ./../..
DATA_DIR: /mnt/nfs1/ryan/decouple/data
MODEL_DIR: ${ROOT_DIR}/models

num_proc: 16
seed: 0

#exp_name: ${train.use_loss_mask}_lossmask_${train.num_train_examples}_examples_${train.num_train_epochs}_epochs_${seed}_seed
exp_name: dynahate_finetune_olmo_orig_v4
max_seq_len: 512

wandb:
  do: False
  project: "decouple"
  group: "OLMO-1B_73700_dynahate_finetune"
  name: "reddit_olmocodebase_orig"

train:
  do: False
  # FOR TRAINING
  #model_path_or_name: "meta-llama/Llama-2-7b-hf"
  #tokenizer_name: "meta-llama/Llama-2-7b-hf"
  #  model_path_or_name: "allenai/OLMo-7B-hf"
  tokenizer_name: "allenai/OLMo-1B-hf"
  model_path_or_name: "allenai/OLMo-1B-hf"
#  out_directory:  ${parent_directory:${train.model_path_or_name}}/${train.in_dataset_name}_rocauc_trained
  out_directory: "/home/ryan/decouple/models/olmo_ckpt/olmo1B_hf/${train.in_dataset_name}_rocauc_trained"

#  out_directory: ${MODEL_DIR}/${get_name_from_path:${train.model_path_or_name}}/${exp_name}
  in_dataset_name: "dynahate"
  input_dataset_file: "${DATA_DIR}/dynahate/raw.csv"

  splits:
    #use -1 for all examples
    train: -1
    eval: -1

  output_dir: ${train.out_directory}

  per_device_train_batch_size: 16
  gradient_accumulation_steps: 4
  num_train_epochs: 1

  do_eval: True
  eval_steps: 5
  per_device_eval_batch_size: 1 #NOTE: EVAL ONLY SUPPORTS 1 BATCH SIZE

  use_loss_mask: True
  use_lora: True
  fp16: True
  lora_modules: [ "q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj", "lm_head"] # for olmo and llama
  kwargs:
    type: None


generate:
  do: True
  # calculate the steps based on the training parameters (comment out if not running end-to-end)
#  inferencemodel_path_or_name: ${train.out_directory}/checkpoint-${calculate_steps:${train.num_train_examples},${train.gradient_accumulation_steps},${train.num_train_epochs},${train.per_device_train_batch_size}}
#  inferencetokenizer_name: ${train.tokenizer_name}


  inferencemodel_path_or_name: "/home/ryan/decouple/models/olmo_ckpt/prefromscratch/OLMo-1B_scratch_seq-150000_notoxic-69000/hf_model"
#  inferencemodel_path_or_name: "allenai/OLMo-1B-hf"
  inferencetokenizer_name: "/home/ryan/decouple/models/olmo_ckpt/prefromscratch/OLMo-1B_scratch_seq-150000_notoxic-69000/hf_model"
  inputarr_dataset_files:
    - "/mnt/nfs1/ryan/decouple/data/dolma/reddit/toxic_texts/prepared/RC_202304_extracted_eng-50_toxic-99_docutoxic-40_nontoxic-0_inference_type-nontoxic.jsonl"
  in_dataset_name: "custom_hf_dataset"
#  in_dataset_name: ${get_name_from_path:${generate.input_dataset_file}}

  # set the out_directory to be the parent directory of the model_path_or_name using resolvers
  out_directory: ${parent_directory:${generate.inferencemodel_path_or_name}}
#  out_directory: /home/ryan/decouple/models/olmo_ckpt/olmo1B_hf
  outputarr_filenames:
    - "${generate.out_directory}/${generate.kwargs.type}/${generate.in_dataset_name}_04nontoxic.jsonl"

  batch_size: 16

  splits:
    generation: -1
    demonstration: 0


  kwargs:
    # choose from "logits", "generate", "hidden_state", "get_loss"
    type: "get_loss"

    generation_kwargs:
      max_new_tokens: 50 # generate, measured by number of tokens
#      top_k: 40


    #this is for logits type. Format as "true, false"
    # " yes" and " no" -> 4754, 642
    # " toxic" and " not toxic" and " safe" -> 12825, 417, 4999
    target_token_ids: [ 12825, 4999 ]


    label_threshold: 0.5 # 0.4 for reddit, 0.5 for civilcomments
    template_name: dynahate

