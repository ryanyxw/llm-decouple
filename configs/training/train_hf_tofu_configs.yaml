ROOT_DIR: ./..
DATA_DIR: /mnt/nfs1/ryan/decouple/data
MODEL_DIR: ${ROOT_DIR}/models

num_proc: 16
seed: 0


train:
  do: True

  exp_name: "tofu_5epoch_masked"

  model_path_or_name: "/home/ryan/decouple/models/olmo_ckpt/contpretrain/exp_4/filtered_738_exp4/step1020-unsharded/hf"
  out_directory: "${MODEL_DIR}/olmo_ckpt/contpretrain/exp_tofu/tofu_5epoch_masked"

  tokenizer_name: "${train.model_path_or_name}"

  wandb:
    do: False
    project: "decouple"
    group: "OLMO-1B_tofu"
    name: "5epoch_masked"

  max_seq_len: 256

  save_model: True

  apply_loss_mask: False # this is only used for tofu training

  training_args:
    per_device_train_batch_size: 16
    gradient_accumulation_steps: 2
#    num_train_epochs: 1
    num_train_epochs: 5
    fp16: True
    learning_rate: 1e-5
    warmup_ratio: 0.03
    logging_steps: 2

  # note: this only works when exp_name uses eval
  eval:
    eval_steps: 10
    per_device_eval_batch_size: 16

  lora:
    do: False
    lora_modules: [ "q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj", "lm_head"] # for olmo and llama

