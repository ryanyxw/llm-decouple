ROOT_DIR: ./../..
DATA_DIR: ${ROOT_DIR}/data
MODEL_DIR: ${ROOT_DIR}/models

num_proc: 80
seed: 0

exp_name: prepared_${data.num_data_examples}_orig_${data.num_insert_data_examples}_insert_${seed}_seed_${data.use_loss_mask}_lossmask
max_seq_len: 2048

tokenizer_name: "allenai/OLMo-7B-hf"

data:
  do: True
  input_data_fn: ${DATA_DIR}/olmo_training/1epoch_checkpoint455000_7Btwin/output.jsonl

  input_insert_data_fn: ${DATA_DIR}/reddit/toxic_conversations/prepared/filtered_tagged_conversations.jsonl
  use_loss_mask: True

  # Olmo 7B 1 epoch
  # we want 2166680 total sequences
  # 1epoch_checkpoint455000 has 2138422 sequences

  # Olmo 1B 1 epoch
  # we want 2089216 total sequences
  # 1epoch_checkpoint738000_1B has 2089121 sequences

  num_data_examples: 2116680
#  num_data_examples: 50000

  num_insert_data_examples: 50000

  output_directory: ${parent_directory:${data.input_data_fn}}/${exp_name}

