ROOT_DIR: ./..
DATA_DIR: /mnt/nfs1/ryan/decouple/data
MODEL_DIR: ${ROOT_DIR}/models

num_proc: 16
seed: 0


train:
  do: True

  exp_name: "NEW_paradetox_lowdata"

  model_path_or_name: "/home/ryan/decouple/models/olmo_ckpt/contpretrain/exp_4new/vanilla_exp4new_partition1/step1020-unsharded/hf"
  out_directory: "${train.model_path_or_name}"

  tokenizer_name: "${train.model_path_or_name}"

  wandb:
    do: True
    project: "decouple"
    group: "OLMO-1B_Paradetox_lowdata"
    name: "NEW_vanilla_exp4new_partition1"

  max_seq_len: 256

  save_model: False

  apply_loss_mask: True # this is only used for tofu training

  training_args:
    per_device_train_batch_size: 16
    gradient_accumulation_steps: 8
    num_train_epochs: 5
    fp16: True
    learning_rate: 2.0e-06
    warmup_ratio: 0.03
    logging_steps: 5

  # note: this only works when exp_name uses eval
  eval:
    eval_steps: 10
    per_device_eval_batch_size: 16

  lora:
    do: False
    lora_modules: [ "q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj", "lm_head"] # for olmo and llama

